{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import netCDF4 as nc\n",
    "import os\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latitude_converted(lat: float):\n",
    "    \"\"\"\n",
    "    Convert latitude value to value between 0 and 720 (0.25 degree representation of latitude)\n",
    "    :param lat: The actual latitude we want to covert to the 720 format\n",
    "    \"\"\"\n",
    "    return (90 - lat) * 4\n",
    "\n",
    "def longitude_converted(lon: float):\n",
    "    \"\"\"\n",
    "    Convert longitude value to value between 0 and 1440 (0.25 degree representation of longitude)\n",
    "    :param lon: The actual longitude we want to convert to the 1440 format \n",
    "    \"\"\"\n",
    "    return (-180 - lon) * -4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that loads a netCDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_netcdf_file(path: str):\n",
    "    \"\"\"\n",
    "    Open a netCDF File, NOTE: After being done with the file don't forgoet to use nc_file.close() to close the file.\n",
    "    :param path: The absolute path to the netCDF file.\n",
    "    \"\"\"\n",
    "    nc_file = nc.Dataset(file, 'r')\n",
    "    return nc_file\n",
    "\n",
    "file = \"E:\\\\Project\\\\Urban computing\\\\data\\\\2003\\\\20030801-ESACCI-L4_FIRE-BA-MODIS-fv5.1.nc\"\n",
    "data = load_netcdf_file(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the South korea matrix from the \"burned_area\" variable in the netCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ba_matrix_one_month(nc_file):\n",
    "    \"\"\"\n",
    "    Create the Burned Area matrix for South Korea NOTE: Do not forget to call nc_file.close() after finishing with the file.\n",
    "    :param nc_file: The netCDF Dataset of the monthly data.\n",
    "    \"\"\"\n",
    "    ba_matrix = np.array(nc_file.variables['burned_area'])\n",
    "    \n",
    "    # Convert lat and long to the 720 and 1440 degree format respectively\n",
    "    max_lat_idx = latitude_converted(south_korea_lat_max)\n",
    "    min_lat_idx = latitude_converted(south_korea_lat_min)\n",
    "    max_lon_idx = longitude_converted(south_korea_lon_max)\n",
    "    min_lon_idx = longitude_converted(south_korea_lon_min)\n",
    "\n",
    "    # Create the south korea BA matrix\n",
    "    south_korea_matrix = ba_matrix[:, max_lat_idx:min_lat_idx, min_lon_idx:max_lon_idx]\n",
    "    return south_korea_matrix\n",
    "\n",
    "\n",
    "# south_korea = create_ba_matrix_one_month(data)\n",
    "# south_korea.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine data\n",
    "Create a single dataset which contains the korea BA matrix over all the months of the years range 2001-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(root_folder):\n",
    "    \"\"\"\n",
    "    Combine data from multiple NetCDF files.\n",
    "    :param root_folder: The root folder containing yearly subfolders.\n",
    "    :return: Combined dataset for South Korea.\n",
    "    \"\"\"\n",
    "    combined_data = None\n",
    "\n",
    "    for year_folder in os.listdir(root_folder):\n",
    "        year_path = os.path.join(root_folder, year_folder)\n",
    "\n",
    "        for month_file in os.listdir(year_path):\n",
    "            file_path = os.path.join(year_path, month_file)\n",
    "\n",
    "            # Open the NetCDF file\n",
    "            with nc.Dataset(file_path, 'r') as nc_file:\n",
    "                south_korea_data = create_ba_matrix_one_month(nc_file)\n",
    "\n",
    "                if combined_data is None:\n",
    "                    combined_data = south_korea_data\n",
    "                else:\n",
    "                    # Combine the data along the time dimension (assuming the first dimension is time)\n",
    "                    combined_data = np.concatenate((combined_data, south_korea_data), axis=0)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "# Specify the root folder where your data is stored\n",
    "data_root_folder = 'E:\\\\Project\\\\Urban computing\\\\data'\n",
    "\n",
    "# Call the combine_data function\n",
    "combined_data = combine_data(data_root_folder)\n",
    "\n",
    "# Now, combined_data contains the aggregated burned area data for South Korea from all files.\n",
    "np.save(file=\"combined_data.npy\" , arr=combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = np.load('combined_data.npy')\n",
    "loaded_data.shape\n",
    "loaded_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GeoTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "south_korea_lat_min = 32\n",
    "south_korea_lat_max = 39\n",
    "south_korea_lon_min = 125\n",
    "south_korea_lon_max = 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_long_to_idx(long: float):\n",
    "    if long < 52.999301 or  long > 179.99999999952965:\n",
    "        return ValueError\n",
    "    return (long - 52.999301) / 0.0022457331\n",
    "\n",
    "def pixel_lat_to_idx(latitude: float):\n",
    "    if latitude < 0 or  latitude > 83.00004990518016:\n",
    "        return ValueError\n",
    "    return (latitude - 1.4210854715202004e-14) / 0.0022457331\n",
    "\n",
    "# Function to clip the TIFF file\n",
    "def clip_tiff(file_path, lon_min, lon_max, lat_min, lat_max):\n",
    "    data = rioxarray.open_rasterio(file_path)\n",
    "    clipped_data = data.rio.clip_box(minx=lon_min, miny=lat_min, maxx=lon_max, maxy=lat_max)\n",
    "    return clipped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory containing the folders\n",
    "base_dir = \"E:\\\\Project\\\\Urban computing\\\\data\\\\Pixel Data\"\n",
    "\n",
    "# Dataset to store the clipped data\n",
    "dataset = []\n",
    "\n",
    "# Loop through each year directory\n",
    "for year in os.listdir(base_dir):\n",
    "    year_dir = os.path.join(base_dir, year)\n",
    "\n",
    "    # Loop through each subdirectory in the year directory\n",
    "    for root, dirs, files in os.walk(year_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\"-CL.tif\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                # Extract the 'month' from the directory name if necessary\n",
    "                month = os.path.basename(root)  # or any other logic to determine the month from the folder name\n",
    "\n",
    "                # Clip the data and add to the dataset\n",
    "                clipped_data = clip_tiff(file_path, south_korea_lon_min, south_korea_lon_max, south_korea_lat_min, south_korea_lat_max)\n",
    "                dataset.append((year, month, clipped_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "data_arrays = [item[2] for item in dataset]  # Extract the DataArray from each tuple\n",
    "# print(dataset[150][2])\n",
    "\n",
    "# Stack the arrays along a new axis (0th axis for time)\n",
    "combined_numpy_array = np.stack(data_arrays, axis=0)\n",
    "\n",
    "# Print the shape of the resulting NumPy array\n",
    "print(combined_numpy_array[10, 0])\n",
    "# np.save(arr=combined_numpy_array, file=\"south_korea.npy\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
